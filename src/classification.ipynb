{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"classification.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNkEBONHOkL+0ggXKwuxTsE"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"xe2AC8QzZk_H"},"source":["# Setup"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jsUSpthuTHm","executionInfo":{"status":"ok","timestamp":1630550757747,"user_tz":420,"elapsed":17,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"232ff1d9-22c6-42a8-f5e8-8a00010dfa81"},"source":["# mount drive \n","from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNzhsNC6uZI3","executionInfo":{"status":"ok","timestamp":1630550757748,"user_tz":420,"elapsed":12,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"f19e6229-69a7-467b-f5d2-022fc1581218"},"source":["cd \"/content/gdrive/My Drive/Github/lc-classification\""],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Github/lc-classification\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYIT5CBmYtnZ","executionInfo":{"status":"ok","timestamp":1630550763301,"user_tz":420,"elapsed":5561,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"8a5bb3ad-2940-46d4-ebf5-fe585e1696e8"},"source":["# import libraries\n","import time\n","import pickle\n","import numpy as np\n","import pandas as pd\n","\n","!pip install transformers &> /dev/null\n","import torch\n","from torch.utils.data import DataLoader\n","from sklearn.metrics import accuracy_score\n","from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW "],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.10.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.45)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.6.4)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (5.4.1)\n","Requirement already satisfied: huggingface-hub>=0.0.12 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.16)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.12->transformers) (3.7.4.3)\n","Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"v9IG-AoSr0AX","executionInfo":{"status":"ok","timestamp":1630550763302,"user_tz":420,"elapsed":5,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}}},"source":["# save pickle files\n","def save_pickle(stuff, fileName):\n","    with open(fileName, 'wb') as f:\n","        pickle.dump(stuff, f, pickle.HIGHEST_PROTOCOL)\n","\n","# load pickle files\n","def load_pickle(fileName):\n","    with open(fileName, 'rb') as f:\n","        return pickle.load(f)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"4juKr5LqZCZS","executionInfo":{"status":"ok","timestamp":1630550763302,"user_tz":420,"elapsed":3,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}}},"source":["#-----------------------------------------------------------------\n","#  Class GutenbergDataset\n","#-----------------------------------------------------------------\n","\n","class GutenbergDataset(torch.utils.data.Dataset):\n","    \n","    def __init__(self, encodings, labels):\n","        self.encodings = encodings\n","        self.labels = labels\n","\n","    def __getitem__(self, idx):\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n","        item['labels'] = torch.as_tensor(self.labels[idx], dtype=torch.int64)\n","        return item\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","#-----------------------------------------------------------------\n","#  End of Class GutenbergDataset\n","#-----------------------------------------------------------------"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gtde0cYDscOF","executionInfo":{"status":"ok","timestamp":1630550763625,"user_tz":420,"elapsed":326,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}}},"source":["#-----------------------------------------------------------------\n","#  Class LongformerClassification\n","#-----------------------------------------------------------------\n","\n","class LongformerClassification:\n","\n","    def __init__(self, tokenizer='allenai/longformer-base-4096', model='allenai/longformer-base-4096', num_labels=19):\n","        \n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","        self.tokenizer = LongformerTokenizer.from_pretrained(tokenizer)\n","        self.model = LongformerForSequenceClassification.from_pretrained(model, num_labels=num_labels, output_hidden_states=False).to(self.device)\n","\n","#-----------------------------------------------------------------\n","\n","    def train(self, train_loader, val_loader, save_model_name, max_epoch=3):\n","\n","        optim = AdamW(self.model.parameters(), lr=5e-5)\n","        start = time.time()\n","        mid_prev = start\n","        for epoch in range(max_epoch):\n","            \n","            print(\"--------------------\")\n","            print(\"epoch \" + str(epoch))\n","            train_loss = 0\n","            val_loss = 0\n","            train_acc = 0\n","            val_acc = 0\n","\n","            # train set\n","            self.model.train()\n","            for batch in train_loader:\n","                optim.zero_grad()\n","                input_ids = batch['input_ids'].to(self.device)\n","                attention_mask = batch['attention_mask'].to(self.device)\n","                labels = batch['labels'].to(self.device)\n","                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = outputs[0]\n","                loss.backward()\n","                optim.step()\n","                \n","                # record train loss and acc\n","                train_loss += loss.data.item()\n","                true = labels.tolist()\n","                pred = outputs.logits.argmax(-1).tolist()\n","                train_acc += accuracy_score(true, pred, normalize=False)\n","\n","            print(\n","                \"Train loss:\", round(train_loss/len(train_loader), 4), \"\\t\", \n","                \"Train acc:\", round(train_acc/len(train_loader), 4)\n","            )\n","\n","            # save model\n","            modelName = \"./work/\" + save_model_name + \"-\" + str(round(time.time()))\n","            self.model.save_pretrained(modelName)\n","            #print(\"Saving model \" + str(round(time.time())) + \" ...\")\n","\n","            # validation set\n","            self.model.eval()\n","            with torch.no_grad():\n","                for batch in val_loader:\n","                    input_ids = batch['input_ids'].to(self.device)\n","                    attention_mask = batch['attention_mask'].to(self.device)\n","                    labels = batch['labels'].to(self.device)\n","                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n","                    loss = outputs[0]\n","\n","                    # record val loss and acc\n","                    val_loss += loss.data.item()\n","                    true = labels.tolist()\n","                    pred = outputs.logits.argmax(-1).tolist()\n","                    val_acc += accuracy_score(true, pred, normalize=False)\n","            \n","            # record epoch runtime \n","            mid_curr = time.time()\n","            print(\n","                \"Val loss:\", round(val_loss/len(val_loader), 4), \"\\t\",\n","                \"Val acc:\", round(val_acc/len(val_loader), 4)\n","            )\n","            self._print_time(\"Runtime:\", mid_prev, mid_curr)\n","            mid_prev = mid_curr\n","\n","        print(\"--------------------\")\n","        end = time.time()\n","        self._print_time(\"Total Runtime\", start, end)\n","        print(\"--------------------\")\n","\n","#-----------------------------------------------------------------\n","\n","    def predict(self, test_loader):\n","\n","        test_loss = 0\n","        test_acc = 0\n","\n","        self.model.eval()\n","        with torch.no_grad():\n","            for batch in test_loader:\n","                input_ids = batch['input_ids'].to(self.device)\n","                attention_mask = batch['attention_mask'].to(self.device)\n","                labels = batch['labels'].to(self.device)\n","                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\n","                loss = outputs[0]\n","                \n","                # record test loss and acc\n","                test_loss += loss.data.item()\n","                true = labels.tolist()\n","                pred = outputs.logits.argmax(-1).tolist()\n","                test_acc += accuracy_score(true, pred, normalize=False)\n","\n","        print(\n","            \"Test loss:\", round(test_loss/len(test_loader), 4), \"\\t\", \n","            \"Test acc:\", round(test_acc/len(test_loader), 4), \n","        )\n","\n","#-----------------------------------------------------------------\n","\n","    def _print_time(self, tag, start, end):\n","        print(tag, round((end-start)//3600), \"hr\", round(((end-start)%3600)//60), \"min\",  round((end-start)%60), \"sec\")     \n","\n","#-----------------------------------------------------------------\n","#  End of Class LongformerClassification\n","#-----------------------------------------------------------------"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzheRXViY0uh","executionInfo":{"status":"ok","timestamp":1630552259257,"user_tz":420,"elapsed":1495636,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"5443e721-4b37-4054-ef03-831352e31e30"},"source":["if __name__ == \"__main__\":\n","\n","    # set parameters\n","    BATCH_SIZE = 2\n","\n","    # import data\n","    #train_set = pd.read_json('./data/train_set.json')\n","    #val_set = pd.read_json('./data/val_set.json')\n","    #test_set = pd.read_json('./data/test_set.json')\n","\n","    # transform text to encodings\n","    #train_encodings = tokenizer(list(train_set.X), max_length=2048, truncation=True, padding=True)\n","    #val_encodings = tokenizer(list(val_set.X), max_length=2048, truncation=True, padding=True)\n","    #test_encodings = tokenizer(list(test_set.X), max_length=2048, truncation=True, padding=True)\n","\n","    # saved encodings and labels \n","    #save_pickle(train_encodings, './work/train_encodings.pkl')\n","    #save_pickle(val_encodings, './work/val_encodings.pkl')\n","    #save_pickle(test_encodings, './work/test_encodings.pkl')\n","    #save_pickle(train_set.y_class, './work/train_labels.pkl')\n","    #save_pickle(val_set.y_class, './work/val_labels.pkl')\n","    #save_pickle(test_set.y_class, './work/test_labels.pkl')\n","\n","    # import encodings (X)\n","    train_encodings = load_pickle('work/train_encodings.pkl')\n","    val_encodings = load_pickle('work/val_encodings.pkl')\n","    test_encodings = load_pickle('work/test_encodings.pkl')\n","\n","    # import labels (y)\n","    train_labels = load_pickle('work/train_labels.pkl')\n","    val_labels = load_pickle('work/val_labels.pkl')\n","    test_labels = load_pickle('work/test_labels.pkl')\n","\n","    # create numerical index \n","    class2label = {cls:i for i, cls in enumerate(sorted(list(set(train_labels))))}\n","    label2class = {v:k for k,v in class2label.items()}\n","\n","    # build custom datasets\n","    train_dataset = GutenbergDataset(train_encodings, [class2label[cls] for cls in train_labels])\n","    val_dataset = GutenbergDataset(val_encodings, [class2label[cls] for cls in val_labels])\n","    test_dataset = GutenbergDataset(test_encodings, [class2label[cls] for cls in test_labels])\n","\n","    # build data loaders\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\n","\n","    # execute main\n","    clf = LongformerClassification(tokenizer='allenai/longformer-base-4096', model='work/longformer-class-2048-1630547063', num_labels=19)\n","    # clf.train(train_loader, val_loader, save_model_name=\"longformer-class-2048\", max_epoch=3)\n","    clf.predict(test_loader)\n"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Test loss: 1.6977 \t Test acc: 1.1636\n"]}]}]}