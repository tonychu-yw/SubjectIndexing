{"nbformat":4,"nbformat_minor":2,"metadata":{"colab":{"name":"classification.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm","authorship_tag":"ABX9TyNkEBONHOkL+0ggXKwuxTsE"},"kernelspec":{"name":"python3","display_name":"Python 3.8.5 64-bit ('base': conda)"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","interpreter":{"hash":"0d8bf6801b9e0c14e5026b6134fd4f1a5633696379168a96ecf2f6cc9392dd51"}},"cells":[{"cell_type":"markdown","source":["# Setup"],"metadata":{"id":"xe2AC8QzZk_H"}},{"cell_type":"code","execution_count":1,"source":["# mount drive \r\n","from google.colab import drive\r\n","drive.mount('/content/gdrive')"],"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9jsUSpthuTHm","executionInfo":{"status":"ok","timestamp":1630550757747,"user_tz":420,"elapsed":17,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"232ff1d9-22c6-42a8-f5e8-8a00010dfa81"}},{"cell_type":"code","execution_count":2,"source":["cd \"/content/gdrive/My Drive/Github/lc-classification\""],"outputs":[{"output_type":"stream","name":"stdout","text":["/content/gdrive/My Drive/Github/lc-classification\n"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VNzhsNC6uZI3","executionInfo":{"status":"ok","timestamp":1630550757748,"user_tz":420,"elapsed":12,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"f19e6229-69a7-467b-f5d2-022fc1581218"}},{"cell_type":"code","execution_count":1,"source":["# import libraries\r\n","import time\r\n","import pickle\r\n","import numpy as np\r\n","import pandas as pd\r\n","\r\n","import torch\r\n","from torch.utils.data import DataLoader\r\n","from sklearn.metrics import accuracy_score\r\n","from transformers import LongformerTokenizer, LongformerForSequenceClassification, AdamW "],"outputs":[],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYIT5CBmYtnZ","executionInfo":{"status":"ok","timestamp":1630550763301,"user_tz":420,"elapsed":5561,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"8a5bb3ad-2940-46d4-ebf5-fe585e1696e8"}},{"cell_type":"code","execution_count":2,"source":["# save pickle files\r\n","def save_pickle(stuff, fileName):\r\n","    with open(fileName, 'wb') as f:\r\n","        pickle.dump(stuff, f, pickle.HIGHEST_PROTOCOL)\r\n","\r\n","# load pickle files\r\n","def load_pickle(fileName):\r\n","    with open(fileName, 'rb') as f:\r\n","        return pickle.load(f)"],"outputs":[],"metadata":{"id":"v9IG-AoSr0AX","executionInfo":{"status":"ok","timestamp":1630550763302,"user_tz":420,"elapsed":5,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}}}},{"cell_type":"code","execution_count":3,"source":["#-----------------------------------------------------------------\r\n","#  Class GutenbergDataset\r\n","#-----------------------------------------------------------------\r\n","\r\n","class GutenbergDataset(torch.utils.data.Dataset):\r\n","    \r\n","    def __init__(self, encodings, labels):\r\n","        self.encodings = encodings\r\n","        self.labels = labels\r\n","\r\n","    def __getitem__(self, idx):\r\n","        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\r\n","        item['labels'] = torch.as_tensor(self.labels[idx], dtype=torch.int64)\r\n","        return item\r\n","\r\n","    def __len__(self):\r\n","        return len(self.labels)\r\n","\r\n","#-----------------------------------------------------------------\r\n","#  End of Class GutenbergDataset\r\n","#-----------------------------------------------------------------"],"outputs":[],"metadata":{"id":"4juKr5LqZCZS","executionInfo":{"status":"ok","timestamp":1630550763302,"user_tz":420,"elapsed":3,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}}}},{"cell_type":"code","execution_count":4,"source":["#-----------------------------------------------------------------\r\n","#  Class LongformerClassification\r\n","#-----------------------------------------------------------------\r\n","\r\n","class LongformerClassification:\r\n","\r\n","    def __init__(self, tokenizer='allenai/longformer-base-4096', model='allenai/longformer-base-4096', num_labels=19):\r\n","        \r\n","        self.device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\r\n","        self.tokenizer = LongformerTokenizer.from_pretrained(tokenizer)\r\n","        self.model = LongformerForSequenceClassification.from_pretrained(model, num_labels=num_labels, output_hidden_states=False).to(self.device)\r\n","\r\n","#-----------------------------------------------------------------\r\n","\r\n","    def train(self, train_loader, val_loader, save_model_name, max_epoch=3):\r\n","\r\n","        optim = AdamW(self.model.parameters(), lr=5e-5)\r\n","        start = time.time()\r\n","        mid_prev = start\r\n","        for epoch in range(max_epoch):\r\n","            \r\n","            print(\"--------------------\")\r\n","            print(\"epoch \" + str(epoch))\r\n","            train_loss = 0\r\n","            val_loss = 0\r\n","            train_acc = 0\r\n","            val_acc = 0\r\n","\r\n","            # train set\r\n","            self.model.train()\r\n","            for batch in train_loader:\r\n","                optim.zero_grad()\r\n","                input_ids = batch['input_ids'].to(self.device)\r\n","                attention_mask = batch['attention_mask'].to(self.device)\r\n","                labels = batch['labels'].to(self.device)\r\n","                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\r\n","                loss = outputs[0]\r\n","                loss.backward()\r\n","                optim.step()\r\n","                \r\n","                # record train loss and acc\r\n","                train_loss += loss.data.item()\r\n","                true = labels.tolist()\r\n","                pred = outputs.logits.argmax(-1).tolist()\r\n","                train_acc += accuracy_score(true, pred, normalize=False)\r\n","\r\n","            print(\r\n","                \"Train loss:\", round(train_loss/len(train_loader), 4), \"\\t\", \r\n","                \"Train acc:\", round(train_acc/len(train_loader), 4)\r\n","            )\r\n","\r\n","            # save model\r\n","            modelName = \"./work/\" + save_model_name + \"-\" + str(round(time.time()))\r\n","            self.model.save_pretrained(modelName)\r\n","            #print(\"Saving model \" + str(round(time.time())) + \" ...\")\r\n","\r\n","            # validation set\r\n","            self.model.eval()\r\n","            with torch.no_grad():\r\n","                for batch in val_loader:\r\n","                    input_ids = batch['input_ids'].to(self.device)\r\n","                    attention_mask = batch['attention_mask'].to(self.device)\r\n","                    labels = batch['labels'].to(self.device)\r\n","                    outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\r\n","                    loss = outputs[0]\r\n","\r\n","                    # record val loss and acc\r\n","                    val_loss += loss.data.item()\r\n","                    true = labels.tolist()\r\n","                    pred = outputs.logits.argmax(-1).tolist()\r\n","                    val_acc += accuracy_score(true, pred, normalize=False)\r\n","            \r\n","            # record epoch runtime \r\n","            mid_curr = time.time()\r\n","            print(\r\n","                \"Val loss:\", round(val_loss/len(val_loader), 4), \"\\t\",\r\n","                \"Val acc:\", round(val_acc/len(val_loader), 4)\r\n","            )\r\n","            self._print_time(\"Runtime:\", mid_prev, mid_curr)\r\n","            mid_prev = mid_curr\r\n","\r\n","        print(\"--------------------\")\r\n","        end = time.time()\r\n","        self._print_time(\"Total Runtime\", start, end)\r\n","        print(\"--------------------\")\r\n","\r\n","#-----------------------------------------------------------------\r\n","\r\n","    def predict(self, test_loader):\r\n","\r\n","        test_loss = 0\r\n","        test_acc = 0\r\n","\r\n","        self.model.eval()\r\n","        with torch.no_grad():\r\n","            for batch in test_loader:\r\n","                input_ids = batch['input_ids'].to(self.device)\r\n","                attention_mask = batch['attention_mask'].to(self.device)\r\n","                labels = batch['labels'].to(self.device)\r\n","                outputs = self.model(input_ids, attention_mask=attention_mask, labels=labels)\r\n","                loss = outputs[0]\r\n","                \r\n","                # record test loss and acc\r\n","                test_loss += loss.data.item()\r\n","                true = labels.tolist()\r\n","                pred = outputs.logits.argmax(-1).tolist()\r\n","                test_acc += accuracy_score(true, pred, normalize=False)\r\n","\r\n","        print(\r\n","            \"Test loss:\", round(test_loss/len(test_loader), 4), \"\\t\", \r\n","            \"Test acc:\", round(test_acc/len(test_loader), 4), \r\n","        )\r\n","\r\n","#-----------------------------------------------------------------\r\n","\r\n","    def _print_time(self, tag, start, end):\r\n","        print(tag, round((end-start)//3600), \"hr\", round(((end-start)%3600)//60), \"min\",  round((end-start)%60), \"sec\")     \r\n","\r\n","#-----------------------------------------------------------------\r\n","#  End of Class LongformerClassification\r\n","#-----------------------------------------------------------------"],"outputs":[],"metadata":{"id":"Gtde0cYDscOF","executionInfo":{"status":"ok","timestamp":1630550763625,"user_tz":420,"elapsed":326,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}}}},{"cell_type":"code","execution_count":6,"source":["if __name__ == \"__main__\":\r\n","\r\n","    # set parameters\r\n","    BATCH_SIZE = 2\r\n","\r\n","    # import data\r\n","    #train_set = pd.read_json('./data/train_set.json')\r\n","    #val_set = pd.read_json('./data/val_set.json')\r\n","    #test_set = pd.read_json('./data/test_set.json')\r\n","\r\n","    # transform text to encodings\r\n","    #train_encodings = tokenizer(list(train_set.X), max_length=2048, truncation=True, padding=True)\r\n","    #val_encodings = tokenizer(list(val_set.X), max_length=2048, truncation=True, padding=True)\r\n","    #test_encodings = tokenizer(list(test_set.X), max_length=2048, truncation=True, padding=True)\r\n","\r\n","    # saved encodings and labels \r\n","    #save_pickle(train_encodings, './work/train_encodings.pkl')\r\n","    #save_pickle(val_encodings, './work/val_encodings.pkl')\r\n","    #save_pickle(test_encodings, './work/test_encodings.pkl')\r\n","    #save_pickle(train_set.y_class, './work/train_labels.pkl')\r\n","    #save_pickle(val_set.y_class, './work/val_labels.pkl')\r\n","    #save_pickle(test_set.y_class, './work/test_labels.pkl')\r\n","\r\n","    # import encodings (X)\r\n","    train_encodings = load_pickle('work/train_encodings.pkl')\r\n","    val_encodings = load_pickle('work/val_encodings.pkl')\r\n","    test_encodings = load_pickle('work/test_encodings.pkl')\r\n","\r\n","    # import labels (y)\r\n","    train_labels = load_pickle('work/train_labels.pkl')\r\n","    val_labels = load_pickle('work/val_labels.pkl')\r\n","    test_labels = load_pickle('work/test_labels.pkl')\r\n","\r\n","    # create numerical index \r\n","    class2label = {cls:i for i, cls in enumerate(sorted(list(set(train_labels))))}\r\n","    label2class = {v:k for k,v in class2label.items()}\r\n","\r\n","    # build custom datasets\r\n","    train_dataset = GutenbergDataset(train_encodings, [class2label[cls] for cls in train_labels])\r\n","    val_dataset = GutenbergDataset(val_encodings, [class2label[cls] for cls in val_labels])\r\n","    test_dataset = GutenbergDataset(test_encodings, [class2label[cls] for cls in test_labels])\r\n","\r\n","    # build data loaders\r\n","    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n","    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n","    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)\r\n","\r\n","    # execute main\r\n","    #clf = LongformerClassification(tokenizer='allenai/longformer-base-4096', model='work/longformer-class-2048-1630547063', num_labels=19)\r\n","    # clf.train(train_loader, val_loader, save_model_name=\"longformer-class-2048\", max_epoch=3)\r\n","    #clf.predict(test_loader)\r\n"],"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: './work/train_encodings.pkl'","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32m<ipython-input-6-7f78559665df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;31m# import encodings (X)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m     \u001b[0mtrain_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./work/train_encodings.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m     \u001b[0mval_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./work/val_encodings.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[0mtest_encodings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./work/test_encodings.pkl'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;32m<ipython-input-2-ab7a07a5f140>\u001b[0m in \u001b[0;36mload_pickle\u001b[1;34m(fileName)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# load pickle files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_pickle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfileName\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './work/train_encodings.pkl'"]}],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AzheRXViY0uh","executionInfo":{"status":"ok","timestamp":1630552259257,"user_tz":420,"elapsed":1495636,"user":{"displayName":"Tony Chu","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gg34pG5yft_9twqehpodxKWt7CLMVt1kRnlSI2U=s64","userId":"04992041947822798791"}},"outputId":"5443e721-4b37-4054-ef03-831352e31e30"}}]}